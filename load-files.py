# -*- coding: utf-8 -*-
# process_data_v2_refatorado.py - Programa de Processamento em Lote e Carregamento para a Staging Area
# ATUALIZA√á√ÉO: Agora utiliza uma PASTA_BASE pr√©-definida para todos os logs e para a busca de dados JSON.
# ATUALIZA√á√ÉO 2: O log de controle agora salva o caminho relativo do arquivo (pasta_imediata/arquivo.json).
import os
import json
import re
import sys 
import logging
from datetime import datetime
from typing import List, Dict, Any, Tuple, Set

# Presume-se que este m√≥dulo lida com a inser√ß√£o no banco de dados.
# O caminho de importa√ß√£o deve estar correto no ambiente de execu√ß√£o.
from data.db_insert import inserir_dados_lote 

# --- Configura√ß√µes de Arquivos ---
# üåü PASTA_BASE DEFINITIVA: O script usar√° este caminho absoluto para buscar JSONs e salvar logs.
PASTA_BASE = r"D:\Sincronizado\tecnologia\data\stj-files" 

LOG_FILE_NAME = r"D:\Sincronizado\tecnologia\data\stj-files\upload_control.log" 
ERROR_LOG_FILE_NAME = r"D:\Sincronizado\tecnologia\data\stj-files\upload_status.log" 

DEBUG_MODE = True 
BATCH_SIZE = 1000 
TABLE_NAME = "staging_data" # Nome da tabela de destino

# Vari√°veis globais para os caminhos completos (definidas em tempo de execu√ß√£o)
LOG_FILE = "" 
ERROR_LOG_FILE = "" 

# Lista de campos que s√£o do tipo TEXT na tabela de origem (Staging) e N√ÉO DEVEM SER TRUNCADOS.
FIELDS_TO_KEEP_LONG = [
    "descricaoClasse", "ementa", "decisao", "jurisprudenciaCitada", 
    "informacoesComplementares", "notas", "termosAuxiliares", 
    "teseJuridica", "referenciasLegislativas", "acordaosSimilares", "tema"
]

# Campos obrigat√≥rios. Se um desses for None ap√≥s o tratamento, o registro √© descartado.
REQUIRED_FIELDS = ["id_origem"] 

# --- Configura√ß√£o do Logging (A configura√ß√£o base ser√° feita em uma fun√ß√£o) ---
logger = logging.getLogger(__name__)

def _setup_environment(base_path: str):
    """ 
    Configura a PASTA_BASE global, cria a estrutura de pastas e configura o logger
    para salvar o log de erros DENTRO da pasta base.
    """
    global PASTA_BASE, LOG_FILE, ERROR_LOG_FILE
    
    # PASTA_BASE √© reatribu√≠da aqui apenas para garantir que a fun√ß√£o use o path validado
    PASTA_BASE = base_path
    
    # 1. Cria a pasta base se ela n√£o existir
    if not os.path.exists(PASTA_BASE):
        try:
            os.makedirs(PASTA_BASE)
        except OSError as e:
            # N√£o usa logger aqui porque o logger pode n√£o estar configurado corretamente
            print(f"Erro Cr√≠tico: N√£o foi poss√≠vel criar a pasta base '{PASTA_BASE}': {e}")
            sys.exit(1)
            
    # 2. Define os caminhos completos
    LOG_FILE = os.path.join(PASTA_BASE, LOG_FILE_NAME)
    ERROR_LOG_FILE = os.path.join(PASTA_BASE, ERROR_LOG_FILE_NAME)

    # 3. Configura o Logger para usar o caminho do log de erro
    # Limpa handlers existentes para reconfigurar (garante que n√£o haja duplica√ß√£o)
    while logger.handlers:
        logger.handlers.pop()
    
    logger.setLevel(logging.INFO)
    
    # Adiciona StreamHandler para logs informativos no console
    logger.addHandler(logging.StreamHandler(sys.stdout))
    
    # Adiciona FileHandler para logs de erro e status (dentro da PASTA_BASE)
    error_file_handler = logging.FileHandler(ERROR_LOG_FILE, mode='a', encoding='utf-8')
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s - %(name)s')
    error_file_handler.setFormatter(formatter)
    logger.addHandler(error_file_handler)
    
    logger.info(f"Ambiente configurado. Logs de status em: {ERROR_LOG_FILE}")

# --- Fun√ß√µes de Log de Arquivo (Usando vari√°veis globais configuradas) ---

# --- ALTERA√á√ÉO ---
# Nova fun√ß√£o auxiliar para obter o caminho relativo para o log.
def _get_relative_log_path(full_path: str, base_folder: str) -> str:
    """
    Retorna o caminho do arquivo relativo √† pasta base.
    Ex: 'subpasta/arquivo.json'
    """
    return os.path.relpath(full_path, base_folder)

def carregar_log() -> Set[str]:
    """ 
    Carrega o log de arquivos j√° processados. O log agora cont√©m caminhos relativos.
    """
    if not LOG_FILE:
        logger.error("LOG_FILE n√£o configurado. O setup falhou.")
        return set()
        
    if os.path.exists(LOG_FILE):
        try:
            with open(LOG_FILE, "r", encoding="utf-8") as log:
                # O log agora armazena o caminho RELATIVO (ex: subpasta/arquivo.json)
                return set(log.read().splitlines())
        except IOError as e:
            logger.error(f"Erro ao carregar log de arquivos: {e}")
            return set()
    return set()

def atualizar_log(arquivo: str):
    """ 
    --- ALTERA√á√ÉO ---
    Atualiza o log com o caminho relativo do arquivo processado com sucesso.
    'arquivo' ainda √© o caminho completo, a convers√£o √© feita aqui.
    """
    if not LOG_FILE:
        logger.error("LOG_FILE n√£o configurado.")
        return

    try:
        # Converte o caminho completo para o formato relativo antes de salvar
        log_entry = _get_relative_log_path(arquivo, PASTA_BASE)
        
        with open(LOG_FILE, "a", encoding="utf-8") as log:
            log.write(f"{log_entry}\n")
    except IOError as e:
        logger.error(f"Erro ao atualizar log para o arquivo {arquivo}: {e}")

# --- Fun√ß√µes de SQL (Sem altera√ß√µes) ---

def sanitize_sql_value(value: Any) -> str:
    """ 
    Formata um valor Python para ser usado diretamente em uma string SQL.
    Adiciona aspas simples para strings e NULL para None.
    """
    if value is None:
        return "NULL"
    if isinstance(value, (int, float)):
        return str(value)
    if isinstance(value, list):
        # Trata listas serializando para JSON string
        value = json.dumps(value, ensure_ascii=False)
        
    # Trata strings, escapando aspas simples e envolvendo em aspas simples
    if isinstance(value, str):
        # Substitui ' por '' (padr√£o SQL para aspas simples em string)
        safe_value = value.replace("'", "''") 
        return f"'{safe_value}'"
        
    return "NULL" # Caso o tipo n√£o seja reconhecido

def gerar_sql_insert_lote(registros: List[Dict[str, Any]]) -> str:
    """ 
    Gera uma string SQL de INSERT com m√∫ltiplos VALUES para fins de log de erro.
    """
    if not registros:
        return ""
    
    # Usa as chaves do primeiro registro como base para a ordem das colunas
    colunas = registros[0].keys()
    colunas_sql = ", ".join(colunas)
    
    values_list = []
    
    for registro in registros:
        valores = []
        for coluna in colunas:
            # Garante que a ordem dos valores √© a mesma ordem das colunas
            valores.append(sanitize_sql_value(registro.get(coluna)))
        
        values_list.append(f"({', '.join(valores)})")
        
    # Constr√≥i o comando INSERT completo
    sql = (
        f"INSERT INTO {TABLE_NAME} ({colunas_sql})\n"
        f"VALUES\n"
        f"{',\n'.join(values_list)};"
    )
    
    return sql

# --- Fun√ß√µes de Limpeza e Valida√ß√£o (Sem altera√ß√µes) ---

def limpar_texto(valor: Any) -> Any:
    """ Remove espa√ßos extras, quebras de linha, tabs e normaliza espa√ßos. """
    if isinstance(valor, str):
        # Substitui quebras de linha/tabs por um √∫nico espa√ßo
        valor = re.sub(r"[\r\n\t]+", " ", valor)
        # Normaliza m√∫ltiplos espa√ßos para um √∫nico
        valor = re.sub(r"\s{2,}", " ", valor)
        return valor.strip()
    return valor 

def extrair_data(valor: Any) -> str:
    """ 
    Tenta converter datas em v√°rios formatos comuns para o formato SQL YYYY-MM-DD.
    """
    data_bruta = str(valor).strip()
    if not data_bruta:
        return ""

    formatos = [
        "%d/%m/%Y", "%d-%m-%Y", "%Y%m%d", "%Y-%m-%d", "%Y/%m/%d"
    ]
    
    try:
        data_para_tentar = data_bruta
        if "DATA:" in data_bruta.upper():
            data_para_tentar = data_bruta.split("DATA:")[1].strip().split(" ")[0]

        for fmt in formatos:
            try:
                data_obj = datetime.strptime(data_para_tentar, fmt)
                return data_obj.strftime("%Y-%m-%d")
            except ValueError:
                continue
        
        return data_bruta

    except Exception:
        return data_bruta
    
def validar_inteiro(valor: Any) -> int | None:
    """ Valida e converte um valor para inteiro. """
    try:
        if valor is None or (isinstance(valor, str) and not valor.strip()):
            return None
            
        if isinstance(valor, str):
            valor = valor.strip()
            
        return int(float(valor))

    except (ValueError, TypeError) as e:
        if DEBUG_MODE:
            logger.warning(f"Falha na convers√£o para inteiro ('{valor}'). Erro: {e}")
        return None

def tratar_dados(dados: Dict[str, Any]) -> Dict[str, Any] | None:
    """ 
    Trata os dados, aplica as convers√µes de tipo, truncamento e valida√ß√£o de obrigatoriedade.
    """
    dados_padronizados = {}
    for key, value in dados.items():
        key_lower = key.lower()
        if key_lower == 'id_registro' or key_lower == 'id':
            dados_padronizados['id_origem'] = value
        else:
            dados_padronizados[key] = value

    dados_tratados = {}
    
    for chave, valor in dados_padronizados.items():
        valor_original = valor 
        
        # 1. Limpeza de Listas e Strings
        if isinstance(valor, list) and all(isinstance(item, str) for item in valor):
            valor = [limpar_texto(item) for item in valor]
        elif isinstance(valor, str):
            valor = limpar_texto(valor)

        # 2. Regras de Valida√ß√£o e Formata√ß√£o (Tipos DDL)
        chave_lower = chave.lower()
        
        # Convers√£o de Datas
        if chave_lower in ("datadecisao", "datadisponibilizacao", "dataregistro", "dataregistrada", "datanascimento", "datarequisicao", "datapublicacao"):
            valor = extrair_data(valor_original if isinstance(valor_original, str) else str(valor))
        
        # Convers√£o de IDs estritamente inteiros (Exemplo: campos com "id" ou "numero" no nome)
        elif "id" in chave_lower and chave_lower not in ("id_origem"): 
             # L√≥gica de valida√ß√£o de IDs para exemplo (pode ser ajustada)
             pass 

        # 3. Truncamento condicional para VARCHARs
        if isinstance(valor, str) and chave not in FIELDS_TO_KEEP_LONG:
            
            # Tratamento espec√≠fico para campos VARCHAR(50)
            if chave_lower in ("id", "numeroprocesso", "numeroregistro") and len(valor) > 50:
                if DEBUG_MODE:
                    logger.debug(f"Campo VARCHAR '{chave}' truncado de {len(valor)} para 50 caracteres.")
                valor = valor[:50]
                
            # Tratamento espec√≠fico para dataPublicacao (VARCHAR(300))
            elif chave == "dataPublicacao" and len(valor) > 300:
                if DEBUG_MODE:
                    logger.debug(f"Campo VARCHAR '{chave}' truncado de {len(valor)} para 300 caracteres.")
                valor = valor[:300]
            
            # Tratamento para outros VARCHARs, limitando a 255
            elif len(valor) > 255:
                if DEBUG_MODE:
                    logger.debug(f"Campo VARCHAR '{chave}' truncado de {len(valor)} para 255 caracteres.")
                valor = valor[:255]

        dados_tratados[chave] = valor
    
    # 4. Valida√ß√£o de Campos Obrigat√≥rios (Rotina de Seguran√ßa)
    for campo_obrigatorio in REQUIRED_FIELDS:
        if campo_obrigatorio not in dados_tratados or dados_tratados[campo_obrigatorio] is None:
            logger.error(f"REGISTRO DESCARTADO: Campo obrigat√≥rio '{campo_obrigatorio}' faltando/nulo. Dados parciais: {json.dumps(dados_tratados)}")
            return None 
            
    return dados_tratados

# --- Fun√ß√µes de Progresso e Coleta ---

def imprimir_progresso(progresso_atual: int, total_arquivos: int, arquivo_atual: str):
    """ Exibe a porcentagem de progresso. """
    if total_arquivos == 0:
        porcentagem = 100.0
    else:
        porcentagem = (progresso_atual / total_arquivos) * 100
        
    sys.stdout.write(f"\rProgresso: {porcentagem:.2f}% ({progresso_atual}/{total_arquivos} arquivos) - √öltimo: {os.path.basename(arquivo_atual):<40}")
    sys.stdout.flush()

def coletar_arquivos(pasta: str) -> Tuple[List[str], int]:
    """ 
    --- ALTERA√á√ÉO ---
    Coleta arquivos JSON, comparando seus caminhos relativos com os do log.
    A 'pasta' √© a PASTA_BASE, onde os arquivos JSON est√£o.
    """
    arquivos_a_processar = []
    arquivos_processados = carregar_log()  # Carrega o set de caminhos relativos processados
    
    arquivos_total = []
    # os.walk percorre a PASTA_BASE em busca dos JSONs
    for raiz, _, arquivos in os.walk(pasta):
        for arquivo in arquivos:
            if arquivo.endswith(".json"):
                caminho_completo_arquivo = os.path.join(raiz, arquivo)
                arquivos_total.append(caminho_completo_arquivo)
    
    for caminho_completo in arquivos_total:
        # Converte o caminho completo do arquivo atual para o formato relativo do log para compara√ß√£o
        caminho_relativo_log = _get_relative_log_path(caminho_completo, pasta)
        
        if caminho_relativo_log not in arquivos_processados:
            # Adiciona o caminho COMPLETO √† lista de processamento, pois √© necess√°rio para abrir o arquivo.
            arquivos_a_processar.append(caminho_completo)

    return arquivos_a_processar, len(arquivos_total)

# --- Fun√ß√£o de Processamento Principal ---
def processar_arquivos(pasta: str):
    """
    Processa todos os arquivos JSON, acumulando e inserindo em lotes.
    A 'pasta' √© a PASTA_BASE configurada.
    """
    
    arquivos_a_processar, total_arquivos_encontrados = coletar_arquivos(pasta)
    total_arquivos = len(arquivos_a_processar)
    arquivos_concluidos = 0
    
    registros_lote = [] 

    if total_arquivos == 0:
        if total_arquivos_encontrados > 0:
            logger.info(f"Todos os {total_arquivos_encontrados} arquivos JSON na pasta j√° foram processados (Verifique {LOG_FILE}).")
        else:
            logger.info(f"Nenhum arquivo JSON encontrado na PASTA BASE '{PASTA_BASE}' para processamento.")
        return

    logger.info(f"Arquivos JSON encontrados: {total_arquivos_encontrados}. Arquivos a processar: {total_arquivos}. Tamanho do Lote: {BATCH_SIZE}")
    
    imprimir_progresso(0, total_arquivos, "In√≠cio...")

    for caminho_arquivo in arquivos_a_processar:
        
        try:
            # A leitura do arquivo √© feita com o caminho_arquivo completo
            with open(caminho_arquivo, "r", encoding="utf-8") as f:
                dados = json.load(f)

            registros = []
            if isinstance(dados, list):
                registros = dados
            elif isinstance(dados, dict):
                registros = [dados]
            else:
                logger.error(f"Formato inv√°lido (nem lista, nem dicion√°rio) no arquivo: {caminho_arquivo}")
                arquivos_concluidos += 1
                imprimir_progresso(arquivos_concluidos, total_arquivos, caminho_arquivo)
                continue

            for elemento in registros:
                elemento_tratado = tratar_dados(elemento)
                
                if elemento_tratado is not None:
                    
                    # L√≥gica para garantir consist√™ncia das chaves no lote
                    if not registros_lote and elemento_tratado:
                        registros_lote.append(elemento_tratado)
                    elif registros_lote and set(elemento_tratado.keys()) == set(registros_lote[0].keys()):
                        registros_lote.append(elemento_tratado)
                    elif registros_lote:
                        # Tenta inserir o lote atual (estrutura consistente) antes de iniciar um novo
                        try:
                            inserir_dados_lote(registros_lote) 
                            registros_lote = [] 
                            registros_lote.append(elemento_tratado) # Come√ßa novo lote
                        except Exception as e:
                            sql_insert_completo = gerar_sql_insert_lote(registros_lote)
                            logger.critical(f"ERRO CR√çTICO DE INSER√á√ÉO EM LOTE. O LOTE N√ÉO FOI INSERIDO. Erro: {e}")
                            logger.critical("-" * 80)
                            logger.critical(f"SQL COMPLETO DO LOTE FALHO (LIMITADO):\n{sql_insert_completo[:10240]}...") 
                            logger.critical("-" * 80)
                            raise # Interrompe o processamento
                            
                    # CHECAGEM DO LOTE E INSER√á√ÉO DE ALTA PERFORMANCE
                    if len(registros_lote) >= BATCH_SIZE:
                        try:
                            inserir_dados_lote(registros_lote) 
                            registros_lote = [] # Reinicia a lista S√ì AP√ìS INSER√á√ÉO BEM-SUCEDIDA
                        except Exception as e:
                            sql_insert_completo = gerar_sql_insert_lote(registros_lote)
                            
                            logger.critical(f"ERRO CR√çTICO DE INSER√á√ÉO EM LOTE. O LOTE N√ÉO FOI INSERIDO. Erro: {e}")
                            logger.critical("-" * 80)
                            logger.critical(f"SQL COMPLETO DO LOTE FALHO (LIMITADO):\n{sql_insert_completo[:10240]}...")
                            logger.critical("-" * 80)
                            raise 

            # Atualiza o log de sucesso (a fun√ß√£o atualizar_log j√° faz a convers√£o para o caminho relativo)
            atualizar_log(caminho_arquivo)
            arquivos_concluidos += 1
            imprimir_progresso(arquivos_concluidos, total_arquivos, caminho_arquivo)

        except json.JSONDecodeError as e:
            logger.error(f"Erro de formato JSON no arquivo {caminho_arquivo}: {e}")
            arquivos_concluidos += 1
            imprimir_progresso(arquivos_concluidos, total_arquivos, caminho_arquivo)
        except Exception as e:
            logger.critical(f"ERRO INESPERADO (Processamento Interrompido) no arquivo {caminho_arquivo}: {e}")
            break 
            
    # FINALIZA√á√ÉO: Insere o lote parcial restante, se houver
    if registros_lote:
        try:
            inserir_dados_lote(registros_lote)
        except Exception as e:
            sql_insert_completo = gerar_sql_insert_lote(registros_lote)
            
            logger.critical(f"ERRO CR√çTICO DE INSER√á√ÉO DO LOTE FINAL ({len(registros_lote)} registros). Estes registros n√£o foram inseridos. Erro: {e}")
            logger.critical("-" * 80)
            logger.critical(f"SQL COMPLETO DO LOTE FINAL FALHO (LIMITADO):\n{sql_insert_completo[:10240]}...")
            logger.critical("-" * 80)
        
    # Imprime 100% final
    sys.stdout.write(f"\rProgresso: 100.00% ({arquivos_concluidos}/{total_arquivos} arquivos) - √öltimo: {'Conclu√≠do!':<40}\n")
    sys.stdout.flush()
    
    logger.info("Processamento de dados conclu√≠do.")

if __name__ == "__main__":
    
    # üåü ALTERA√á√ÉO: O script usa a PASTA_BASE predefinida e remove o input do usu√°rio.
    pasta_base_final = PASTA_BASE 

    # 1. Garante que a pasta base exista e configura os caminhos de log.
    _setup_environment(pasta_base_final)
    
    # 2. Inicia o processamento, usando a PASTA_BASE como root para a busca de JSONs
    if os.path.isdir(PASTA_BASE):
        logger.info(f"Iniciando processamento em lote cont√≠nuo na PASTA BASE: {PASTA_BASE}")
        try:
            # processar_arquivos usa PASTA_BASE como o diret√≥rio raiz para os arquivos JSON.
            processar_arquivos(PASTA_BASE)
        except Exception as e:
            logger.critical(f"Processamento abortado devido a um erro cr√≠tico: {e}")
    else:
        # Esta mensagem s√≥ aparecer√° se a cria√ß√£o da pasta em _setup_environment falhar.
        logger.error(f"O caminho fornecido n√£o √© uma pasta v√°lida ou n√£o p√¥de ser criado: {PASTA_BASE}")